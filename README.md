# Evolutionary NAS
This repository collects recent Nature-inspired algorithms(Evolutionary and swarm)--based Neural Architecture Search (NAS) optimization and provides a summary (Paper and Code). 

Neural Architecture Search (NAS), a subfield of AutoML, aims to  automate the architecture designs of neural networks by optimizing the topology of the networks( how to connect nodes and which operators to choose). NAS is a growing area in deep learning research that aims to deliver better-performing models and applications. It follows three stages:
1. Search space: Layer-based, Block-based, and Cell-based. For instance, Cell-based architecture is represented as a DAG where an edge is an operation (Convolution, pooling, or a skip connection) present in the pre-defined search space. The task is to search for individual operations on each edge. The obtained cell is stacked multiple times to form the final CNN, therefore reusing the same cell architecture throughout the network. This representation is like a single-cell structure, which can have a varying impact on accuracy and latency at each neural network layer. In contrast to cell-based, layer-wise-based searches different operations/block configurations at each layer of the network to obtain more efficient models that are suitable for efficient inference on many devices due to the
regularized topology  [Krishna Teja et al., 2022] (https://doi.org/10.1145/3524500).
2. Search strategy/algorithm, and
3. Evaluation strategy. Lower Fidelity Estimation LFE, which tends to minimize the amount of GPU time required by each candidate through
partial training. 

![Screenshot](NAS-high-level.png)
Fig. 1. Three main components of Neural Architecture Search (NAS) models. (Image source: [Elsken, et al. 2019 ](https://arxiv.org/abs/1808.05377) with customized annotation in red https://lilianweng.github.io/posts/2020-08-06-nas/) 
# <h1 id='Content'>Papers</h1>

<ins> Towards Less Constrained Macro-Neural Architecture Search </ins> [Paper](https://arxiv.org/abs/2203.05508), [Code](https://github.com/VascoLopes/LCMNAS), Date 2023.

Recently, V. Lopes et al. proposed LCMNAS that models the search space as Weighted Directed Graphs with hidden properties aiming to automatically design search spaces without requiring human-specified settings by leveraging information about existing CNNs. The search strategy can perform both micro and macro-architecture search, via evolution, without requiring human-defined restrictions(e.g., heuristic). A mixed-performance strategy is proposed for a fast-generated architecture evaluation. 

 <ins> HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization </ins>[Paper](https://arxiv.org/abs/2211.10250), [Code](https://github.com/ThunderStruct/HiveNAS/), Date 2023. 

HiveNAS is a Swarm optimization-based NAS optimization. Artificial Bee Colony optimization is used. Layer-based Search Space is used to represent the search space. Hence, the main contribution of this paper is regarding search space representation. For memory-efficiency purposes, HiveNAS discards the DAG-encoding concept and instead encodes architectures on the fly, storing each positional data (string-encoded neural architecture) in its corresponding bee memory. During the evaluation phase, the authors applied Lower Fidelity Estimation, which boosts the exploration capabilities of the large and granular search space with minimal compromises. Regarding evaluation, HiveNAS uses LFE Evaluation Strategy. The search strategy is based on the assumption that  2 architectures are neighbors if there is a 1-operation (i.e layer) difference between them. This does not reflect an accurate mapping of position-to-fitness given that two neural topologies with 1-layer difference are most likely not close on the fitness optimization surface. 


<ins> TS-ENAS:Two-Stage Evolution for Cell-based Network Architecture Search </ins>  [Paper](https://arxiv.org/abs/2310.09525), [Code](No Code), Date 2023. 

TS-ENAS features by two-stage search strategy. The cells in the searched neural network structure are fine-tuned to find the best neural network structure. First, an evolutionary algorithm is used to search for the general structure of the network based on the cell-based search space. A novel cell-based search space and effective Double-coding are created to represent various building blocks and neural network architectures in the search space to match this two-stage search technique. In the second stage, a global search for specific operations in the network is performed. TS-ENAS performs cross-mutation and pruning operations on the nodes and edges inside the cell. To accelerate the search process, a weight inheritance method is used: the weights of each new individual are inherited from the cell search space.

 <ins> EF-ENAS: Evolutionary neural architecture search based on evaluation correction and functional units </ins>  [Paper](https://web.xidian.edu.cn/rhshang/files/19Evolutionary%20neural.pdf), [Code](https://github.com/codesl173/EF-ENAS), Date 2022. 

To overcome issues observed in existing Evolutionary NAS algorithms which do not perform well, mainly for crossover operations, due to the diverse structure of neural networks and the difficulty in performance evaluation, EF-ENA is proposed as an evolutionary NAS algorithm based on evaluation correction and functional units. Hence, to select a better parent network for generic operation, the authors propose  an evaluation correction-based mating selection operation (ECMS) to correct the network performance discrimination based on low fidelity accuracy. ECMS evaluates the neural network performance from three aspects: the validation accuracy, the number of parameters, and the structural property of the network. Network architectures with higher validation accuracy and a lower number of parameters are often considered to have better performance. The maximum dropout rate (called the dropout parameter) among the convolutional blocks to evaluate the network performance.

Aiming to enhance the performance of the traditional network architecture crossovers, a functional unit-based crossover operation is proposed. The idea behind this operation is to split the network architecture into different segments according to  functional units and then when the crossover operation is performed between two architectures only functional units are exchanged between these two networks. This approach protects valuable network architecture segments from being destroyed and improves the performance of crossover operations. To boost the diversity of the algorithm, an environmental selection operation based on species protection is developed, which divides the network population into different species based on the individuals' structure and selects them equally. This operation balances environmental selection pressure among different network architectures. Block-based encoding approach is used to design a solution. Hence, a network architecture is composed of three types of building blocks (convolutional, pooling, and fully connected blocks). 

 <ins> G-EA: Efficient Guided Evolution for Neural Architecture Search </ins>  [Paper](https://arxiv.org/abs/2110.15232), [Code](https://github.com/VascoLopes/GEA), Date 2022. 

Similar to the idea presented by NPENAS, G-EA uses a zero-proxy estimator, as a guiding mechanism to the search method,  to evaluate several architectures in each generation at the initialization stage. Zero-proxy estimator, which is based on Jacobian covariance,  reduces running time while not requiring any training to evaluate an architecture. To reduce the search space only the Top P scoring networks are trained and kept for the next generation. An Evolutionary algorithm is applied. The mutation operation is achieved by randomly changing one operation of the architecture by another from the pool of operations.  A new architecture is generated at each cycle by performing operation mutations over the selected parent, which are then scored using the zero-proxy estimator. Only the highest-scoring architecture is kept and added to the population after evaluating its fitness. Aiming to boost the exploration ability, G-EA removes old architecture and keeps younger architectures  that represent new settings evolved by previously acquired knowledge. 

<ins> A hardware-aware framework for accelerating Neural  architecture search  across modalities </ins> [Paper](https://arxiv.org/abs/2205.10358), [Code](https://github.com/IntelLabs/DyNAS-T), Date 2022.

The solution comes from Intel researchers, the implementation is called DyNAS-T (Dynamic Neural Architecture Search Toolkit) is a super-network neural architecture search NAS optimization package. Aiming to obtain hardware-performance trade-offs, the proposed framework, which belongs to the one-shot weight-sharing NAS paradigm, accelerates the post-training sub-network search process( not the optional fine-tuning stage). For this end, multi-objective genetic algorithms and lightly trained objective predictors are used. 

<ins> IntelliSwAS: Optimizing deep neural network architectures using a particle swarm-based approach </ins>[Paper](https://arxiv.org/abs/1909.04977), [Code](https://github.com/huawei-noah/CARS), Date 2021. 

IntelliSwAS is a Swarm optimization-based NAS for CNN architecture optimization. To boost the search speed, the search space is modeled as a DAG structure where vertices represent a computational unit and the data flow is represented as directed edges, which is a cell-based search space. A solution is encoded as a binary number between 0 and $2^N-1$, which encodes a Cell. N is the number of bits required to represent a cell, which computes based on the number of bits m required to represent an operation. In this paper, m is equal to 3 while 8 operations are adopted. The search strategy is an enhancement of the original version of PSO algorithm. For instance, instead of keeping a global best solution/position, each particle has a list of friends to be consulted about their best position. The list of friends can be, for example, fixed according to probability, or a fixed number of friends are selected randomly. Hence, each particle stores its best position and the best position found by its friend particles.

Regarding the **evaluation**, which is the fitness function, the accuracy measure of the full CNN is used. For this end, a zero-cost proxy estimator is applied, which can evaluate and compare 2 cells without needing to train and test the CNN architectures. After the end of each step, IntelliSwAS evaluates some of the best-discovered positions, by training and testing the corresponding CNNs. Hence, a machine learning model is used to evaluate 2 architectures instead of estimating the performance of one architecture. This model, called Directed Acyclic Graph Recurrent Neural Networks (DAGRNNs) attempts to estimate the performance of two CNNs architectures without training and testing these CNNs, by predicting CNN structure. DAGRNNs is a variant of RNN to better represent the structure of a cell. it's composed of an attention layer and an RNN. The attention layer assigns a score to each node from which an edge connects to the current one. These scores are afterward passed through a softmax operation to output weights that will be used to compute an aggregated internal state as the weighted sum of the internal states at the other nodes. When cells that need comparison are encoded through DAGRNN,  and feded to a feed-forward neural network to predict which CNN cell would perform better. 

**Drawback**: These strategies are not clear enough to enhance the convergence of the algorithm while no information about the quality of the position is considered. Exploiting the evaluation value of an architecture can boot the optimized algorithm.


<ins> CARS: Continuous Evolution for Efficient Neural Architecture Search </ins> [Paper](https://arxiv.org/abs/1909.04977), [Code](https://github.com/huawei-noah/CARS), Date 2020. 

The idea behind the Care solution is the continuous search for neural network architectures by maximally utilizing the knowledge learned in the last evolution generation. At the initial stage, SuperNet was initialized with considerable cells and blocks. An individual representing an architecture is obtained from the SuperNet through several benchmark operations (i.e., crossover and mutation). The genetic algorithm is used as an evolutionary algorithm. To reduce the search space, the parameters-sharing strategy is adopted to share  parameters W for different architectures. Computing the gradient (slope) using the chain rule w.t. Wi can be costly. 

<ins> NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search </ins> [Paper](https://arxiv.org/abs/2003.12857), [Code](https://github.com/auroua/NPENASv1), Date 2020. 

In this paper, to enhance the exploration ability of EA algorithm, NPENAS to neural predictors are defined. The first predictor a graph-based uncertainty estimation network as a surrogate model. The second predictor is a graph-based neural network that directly outputs the performance prediction of the input neural architecture. An Evolutionary algorithm is applied. These predictors are used to rank the candidate architectures. The top performance architectures are selected as offspring and evaluated by training and validation. The procedure repeats a given number of times, and the neural predictor is trained from scratch with all the architectures in the pool at each iteration.

# <h1 id='Content'>Open source Framworks</h1>
1. [ArchGym]( https://github.com/srivatsankrishnan/oss-arch-gym)
2. [DyNAS-T](https://github.com/IntelLabs/DyNAS-T)

